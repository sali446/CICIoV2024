{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6b8c456-8b19-4e1e-a5dd-d3b226b2b7b9",
   "metadata": {},
   "source": [
    "# Internet of Vehicles (IoV) Network Packet Analysis (NPA) for Intrusion Detection Systems (IDS) - ETL and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513956c8-67a5-44a6-ba2d-8974574abdd2",
   "metadata": {},
   "source": [
    "This notebook contains the ETL and Feature Engineering portion of the this project. Datasets were combined then split into training and test data to be used in logistic regression and anomaly detection.\n",
    "\n",
    "This section requires the installation of PySpark, FindSpark and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565b4c69-b223-4d51-9529-1ff47ef15b1a",
   "metadata": {},
   "source": [
    "### Importing Libraries and Initiating PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ac0404-54a0-43df-9b92-edd338a1f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13871f0-5edf-4d1a-bac9-3e7f396d902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fc60b48-0844-40e0-874e-0369f1acdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da520911-e291-4b56-a803-efd091ef6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75741a-c9e0-41de-a562-2844644659aa",
   "metadata": {},
   "source": [
    "### Data Loading and Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7300dfc-19ec-4078-8aca-fe94cd910267",
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_files = ['./decimal_IOT_Dataset/decimal_DoS.csv', \\\n",
    "             './decimal_IOT_Dataset/decimal_spoofing-GAS.csv', \\\n",
    "             './decimal_IOT_Dataset/decimal_spoofing-RPM.csv', \\\n",
    "             './decimal_IOT_Dataset/decimal_spoofing-SPEED.csv', \\\n",
    "             './decimal_IOT_Dataset/decimal_spoofing-STEERING_WHEEL.csv']\n",
    "\n",
    "atk_data = pd.concat([pd.read_csv(file) for file in atk_files])\n",
    "atk_data.to_csv('decimal_attack.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63576b61-9ae8-4fe2-89af-38e8153411c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "| ID|DATA_0|DATA_1|DATA_2|DATA_3|DATA_4|DATA_5|DATA_6|DATA_7| label|category|specific_class|\n",
      "+---+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "|291|     0|     0|     0|     0|     0|     0|     0|     0|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "|291|    14|    11|     4|     4|     3|     3|     8|    12|ATTACK|     DoS|           DoS|\n",
      "+---+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack_df = spark.read.csv('./decimal_IOT_Dataset/decimal_attack.csv', header = True)\n",
    "attack_df.createOrReplaceTempView('attack')\n",
    "attack_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe50a4ff-f857-41ee-8aaf-c55b27408473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "|  ID|DATA_0|DATA_1|DATA_2|DATA_3|DATA_4|DATA_5|DATA_6|DATA_7| label|category|specific_class|\n",
      "+----+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "|  65|    96|     0|     0|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "|1068|   132|    13|   160|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 535|   127|   255|   127|   255|   127|   255|   127|   255|BENIGN|  BENIGN|        BENIGN|\n",
      "| 131|    15|   224|     0|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 936|     1|     0|    39|    16|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 359|     0|   128|     0|     0|     0|     1|   227|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 369|    16|   108|     0|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 516|   192|     0|   125|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 609|     0|     0|     9|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "|1071|   125|     4|     0|     2|   113|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "|1085|    20|     0|     0|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "|1086|     0|    55|    16|     0|     0|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 118|    62|   128|    66|   128|    15|   255|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 119|   255|   255|    15|   255|   127|   255|   255|   255|BENIGN|  BENIGN|        BENIGN|\n",
      "| 125|     0|     0|   247|   128|     0|    63|   255|   255|BENIGN|  BENIGN|        BENIGN|\n",
      "| 531|   255|   255|     8|   127|    47|     7|   255|    80|BENIGN|  BENIGN|        BENIGN|\n",
      "| 532|     0|     0|     0|     0|     0|     0|   247|   128|BENIGN|  BENIGN|        BENIGN|\n",
      "| 534|     0|     0|     0|     0|   195|     0|     0|     0|BENIGN|  BENIGN|        BENIGN|\n",
      "| 535|   127|   255|   127|   255|   127|   255|   127|   255|BENIGN|  BENIGN|        BENIGN|\n",
      "|1045|     0|     0|    52|   242|    15|   255|    15|   255|BENIGN|  BENIGN|        BENIGN|\n",
      "+----+------+------+------+------+------+------+------+------+------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benign_df = spark.read.csv('./decimal_IOT_Dataset/decimal_benign.csv', header = True)\n",
    "benign_df.createOrReplaceTempView('benign')\n",
    "benign_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809e12f-7d0e-4259-8645-92accea33787",
   "metadata": {},
   "source": [
    "### Data Cleansing and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f70ef6-098d-41f8-a6f9-6ee923815877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming an existing label column for downstream feature engineering\n",
    "attack_df = attack_df.withColumnRenamed(\"label\", \"string\")\n",
    "benign_df = benign_df.withColumnRenamed(\"label\", \"string\")\n",
    "\n",
    "# dropping irrelevant columns\n",
    "attack_df = attack_df.drop('ID', 'category', 'specific_class')\n",
    "benign_df = benign_df.drop('ID', 'category', 'specific_class')\n",
    "\n",
    "# changing data types of feature columns\n",
    "cols_to_cast = ['DATA_0','DATA_1','DATA_2','DATA_3','DATA_4','DATA_5','DATA_6','DATA_7']\n",
    "for col_name in cols_to_cast:\n",
    "    attack_df = attack_df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    benign_df = benign_df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b533dbf-041c-4079-90e3-b5169b7c8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_0' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_1' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_2' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_3' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_4' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_5' has 0 null values.\n",
      "Column 'DATA_6' has 0 null values.\n",
      "Column 'DATA_7' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'string' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_0' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_1' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_2' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_3' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_4' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_5' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_6' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'DATA_7' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'string' has 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# determining presence of null values\n",
    "for col in attack_df.columns:\n",
    "    null_count = attack_df.filter(attack_df[col].isNull()).count()\n",
    "    print(f\"Column '{col}' has {null_count} null values.\")\n",
    "\n",
    "for col in benign_df.columns:\n",
    "    null_count = benign_df.filter(benign_df[col].isNull()).count()\n",
    "    print(f\"Column '{col}' has {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e998a-a70e-4a7e-8d3d-33cd1538e072",
   "metadata": {},
   "source": [
    "### Test and Train Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5db928f-b8ed-45bc-91bb-72b55268547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and combine train and test data sets\n",
    "split = [0.7,0.3]\n",
    "atk_dfs = attack_df.randomSplit(split)\n",
    "benign_dfs = benign_df.randomSplit(split)\n",
    "\n",
    "train_df = benign_dfs[0].union(atk_dfs[0])\n",
    "test_df = benign_dfs[1].union(atk_dfs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4d2dd-356c-495a-a14d-9537ebff6fce",
   "metadata": {},
   "source": [
    "### Feature Engineering - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f638c9-87c2-45d4-ab32-c268a6d785c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline creation\n",
    "indexer = StringIndexer(inputCol = 'string', outputCol = 'label')\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=['DATA_0','DATA_1','DATA_2','DATA_3','DATA_4','DATA_5','DATA_6','DATA_7'\\\n",
    "                                            ], outputCol='features')\n",
    "\n",
    "lr = LogisticRegression(maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)\n",
    "\n",
    "pipeline = Pipeline(stages = [indexer, vectorAssembler, lr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32a37d-f81d-4d79-b2b8-4589dd9bf2ae",
   "metadata": {},
   "source": [
    "### Feature Engineering - Anomaly Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "252cfe08-8349-4d61-9262-28370ce9f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# transform label from string to integer value\n",
    "indexer = StringIndexer(inputCol = 'string', outputCol = 'label')\n",
    "\n",
    "train_data = indexer.fit(train_df).transform(train_df)\n",
    "test_data = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "# remove previous string label\n",
    "train_data = train_data.drop('string')\n",
    "test_data = test_data.drop('string')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3750e383-764e-4217-bc51-f71f1cc3204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandas_df = train_data.toPandas()\n",
    "\n",
    "# Convert Pandas DataFrame to NumPy array\n",
    "numpy_array = pandas_df.to_numpy()\n",
    "\n",
    "X_train = numpy_array[:, :-1]  # Input features (all columns except the last one)\n",
    "y_train = numpy_array[:, -1]   # Target variable (last column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027acb9c-4b3b-412b-8eba-d25c8e2d3f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandas_df = test_data.toPandas()\n",
    "\n",
    "# Convert Pandas DataFrame to NumPy array\n",
    "numpy_array = pandas_df.to_numpy()\n",
    "\n",
    "X_test = numpy_array[:, :-1]  # Input features (all columns except the last one)\n",
    "y_test = numpy_array[:, -1]   # Target variable (last column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d3ca24-f5a9-43fd-9ee9-dd05532351d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004acfb8-e6fd-4cb6-bb28-092ff64491c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f00e0aa5-2f71-4d02-8c7e-7420bf5e15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: \n",
      "rows: 984407\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [194.   1.   2. ...   5. 138.  34.]\n",
      " [194.   1.   2. ...   5. 138.  34.]\n",
      " [194.   1.   2. ...   5. 138.  34.]]\n",
      "\n",
      "\n",
      "y_train: \n",
      "rows: 984407\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "\n",
      "\n",
      "X_test: \n",
      "rows: 423812\n",
      "[[  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " [  0.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [194.   1.   2. ...   5. 138.  34.]\n",
      " [194.   1.   2. ...   5. 138.  34.]\n",
      " [194.   1.   2. ...   5. 138.  34.]]\n",
      "\n",
      "\n",
      "y_test: \n",
      "rows: 423812\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \")\n",
    "print(\"rows:\", len(X_train))\n",
    "print(X_train)\n",
    "print(\"\\n\")\n",
    "print(\"y_train: \")\n",
    "print(\"rows:\", len(y_train))\n",
    "print(y_train)\n",
    "print(\"\\n\")\n",
    "print(\"X_test: \")\n",
    "print(\"rows:\", len(X_test))\n",
    "print(X_test)\n",
    "print(\"\\n\")\n",
    "print(\"y_test: \")\n",
    "print(\"rows:\", len(y_test))\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "446ba465-32d0-46fa-87bd-213d1975a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
